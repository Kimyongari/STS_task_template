{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a03da45-be3c-4a1f-9a09-65563bede26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import torchmetrics\n",
    "from scipy.stats import pearsonr\n",
    "import gc\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72ebbff7-958e-4ff5-afaf-4dfd008b36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, tokenizer, max_length, mode = 'train'):\n",
    "        self.inputs = inputs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = self.inputs.iloc[idx] \n",
    "\n",
    "        text = '[SEP]'.join([t[col] for col in self.text_columns])\n",
    "        output = self.tokenizer(text,\n",
    "                                padding='max_length',\n",
    "                                max_length=self.max_length,\n",
    "                                truncation=True)\n",
    "\n",
    "        datas = torch.tensor(output['input_ids'], dtype = torch.long)\n",
    "        attn = torch.tensor(output['attention_mask'], dtype = torch.long)\n",
    "        # type_ids = torch.tensor(output['token_type_ids'], dtype = torch.long)\n",
    "        if self.mode == 'train':\n",
    "            labels = t['label']\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                    #   'token_type_ids' : type_ids,\n",
    "                      'labels' : labels}\n",
    "            return output\n",
    "        else:\n",
    "            output = {'input_ids' : datas,\n",
    "                      'attention_mask' : attn,\n",
    "                    #   'token_type_ids' : type_ids\n",
    "                    }\n",
    "            return output\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e5be728-d248-4109-929d-97fee7ef0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model_preds):\n",
    "    preds, labels = model_preds\n",
    "    preds = torch.tensor(preds, dtype = torch.float32).squeeze(-1)\n",
    "    labels = torch.tensor(labels, dtype = torch.float32).squeeze(-1)\n",
    "    pear = torchmetrics.PearsonCorrCoef()\n",
    "    pearson = pear(preds, labels)\n",
    "    return {'pearson' : pearson.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "301dbbea-4e04-4a74-a3cc-0a4bbfe1a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(train_route = None, tokenizer = None, max_length = None, valid_route = None, test_route = None, k = None, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        # train_route, valid_route, tokenizer, max_length, mode = 'train'\n",
    "        train_data = pd.read_csv(train_route)\n",
    "        valid_data = pd.read_csv(valid_route)\n",
    "        train_dataset = Dataset(train_data, tokenizer = tokenizer, max_length = max_length)\n",
    "        valid_dataset = Dataset(valid_data, tokenizer = tokenizer, max_length = max_length)\n",
    "        return train_dataset, valid_dataset\n",
    "        \n",
    "    if mode == 'k_fold_train':\n",
    "        # train_route, k, tokenizer, max_length, mode = 'k_fold_train'\n",
    "        df = pd.DataFrame()\n",
    "        for r in train_route:\n",
    "            df = pd.concat([df, pd.read_csv(r)])\n",
    "    \n",
    "        kfold = KFold(n_splits = k, shuffle = True, random_state = 42)\n",
    "        train_list = []\n",
    "        valid_list = []\n",
    "        for train_idx, val_idx in kfold.split(df):\n",
    "            train = df.iloc[train_idx]\n",
    "            valid = df.iloc[val_idx]\n",
    "            train_dataset = Dataset(train, tokenizer = tokenizer, max_length = max_length)\n",
    "            valid_dataset = Dataset(valid, tokenizer = tokenizer, max_length = max_length)\n",
    "            train_list.append(train_dataset)\n",
    "            valid_list.append(valid_dataset)\n",
    "        return train_list, valid_list\n",
    "        \n",
    "    if mode == 'test':\n",
    "        # test_route, tokenizer, max_length, mode = 'test'\n",
    "        test_data = pd.read_csv(test_route)\n",
    "        test_dataset = Dataset(test_data, tokenizer = tokenizer, max_length = max_length, mode = 'test')\n",
    "        return test_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2d6e4e8-caf4-4e13-8e64-e14b6a4a4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(MyModel, self).__init__()\n",
    "        if model_name == \"snunlp/KR-ELECTRA-discriminator\":\n",
    "            print('found KR-ELECTRA')\n",
    "            self.model = ElectraModel.from_pretrained(\"snunlp/KR-ELECTRA-discriminator\")\n",
    "        else:\n",
    "            self.model = transformers.AutoModel.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "        self.cnn_block1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=768, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.cnn_block2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, 1)  \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, \n",
    "                # token_type_ids,\n",
    "                  labels = None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                            # token_type_ids=token_type_ids\n",
    "                            )\n",
    "        \n",
    "        output = output.last_hidden_state.permute(0, 2, 1)  \n",
    "        cnn_output = self.cnn_block1(output)  # Shape: (B, 768, L) >  (B, 256, L) > (B, 128, L/2)\n",
    "        \n",
    "        cnn_output = self.cnn_block2(cnn_output)  # Shape: (B, 128, L/2) > (B, 128, L) > (B, 128, 1)\n",
    "        cnn_output = cnn_output.view(cnn_output.size(0), -1)  # Shape: (B, 128)\n",
    "        output = self.output_layer(cnn_output).squeeze(-1) # Shape: (B)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(output, labels.float())\n",
    "            return {'output' : output, 'loss' : loss}\n",
    "        else:  \n",
    "            return {'output' : output}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aac34709-06fc-468d-a241-dc488d25f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    model = MyModel(model_name)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    datacollator = DataCollatorWithPadding(tokenizer = tokenizer,padding = True, return_tensors = 'pt')\n",
    "    return model, tokenizer, datacollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18ed93f2-72ae-4f3c-9200-aed60c7d53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, args, training_args, kfold = False):\n",
    "    model, tokenizer, data_collator = get_model(model_name)\n",
    "    if kfold:\n",
    "        train_l, eval_l = make_dataset(train_route = args.data_routes['train_routes'], \n",
    "                                    k = args.kf,\n",
    "                                    tokenizer = tokenizer,\n",
    "                                    max_length = args.max_length,\n",
    "                                    mode = 'k_fold_train')\n",
    "                                    \n",
    "        for i in range(args.kf):\n",
    "            print(f'------fold : {i} --- model_name : {model_name}')\n",
    "            trainer = Trainer(model =  model,\n",
    "                                tokenizer = tokenizer,\n",
    "                                args = training_args,\n",
    "                                train_dataset = train_l[i],\n",
    "                                eval_dataset = eval_l[i],\n",
    "                                compute_metrics = compute_metrics,\n",
    "                                data_collator = data_collator,\n",
    "                                 )\n",
    "            trainer.train()\n",
    "        trainer.save_model(f'results/best_model_{model_name}')\n",
    "        preds = test(trainer, args)\n",
    "        return preds\n",
    "    else:\n",
    "        train_dataset, valid_dataset = make_dataset(train_route = args.data_routes['train_routes'][0],\n",
    "                                                    valid_route = args.data_routes['valid_route'],\n",
    "                                                    tokenizer = tokenizer,\n",
    "                                                    max_length = args.max_length,\n",
    "                                                    mode = 'train')\n",
    "        trainer = Trainer(model =  model,\n",
    "                          tokenizer = tokenizer,\n",
    "                          args = training_args,\n",
    "                          train_dataset = train_dataset,\n",
    "                          eval_dataset = valid_dataset,\n",
    "                          compute_metrics = compute_metrics,\n",
    "                          data_collator = data_collator,\n",
    "                                 )\n",
    "        print(f'------- model_name : {model_name}')\n",
    "        trainer.train()\n",
    "        trainer.save_model(f'results/best_model_{model_name}')\n",
    "        preds = test(trainer, args)\n",
    "        return preds\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1546a11e-fb54-46b4-ab6c-fdfe142e6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(trainer, args):\n",
    "    preds = pd.DataFrmae()\n",
    "    \n",
    "    test_dataset = make_dataset(test_route = args.test_route, \n",
    "                                tokenzier = trainer.tokenizer,\n",
    "                                max_length = args.max_length, \n",
    "                                mode = 'test')\n",
    "    \n",
    "    preds = trainer.predict(test_dataset)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cbdc0-ad7b-4027-9f6e-36620d3d0766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "              'klue/roberta-small'\n",
    "            # \"klue/roberta-large\",\n",
    "            # 'monologg/koelectra-base-v3-discriminator',\n",
    "            # \"snunlp/KR-ELECTRA-discriminator\",\n",
    "            # 'Alibaba-NLP/gte-multilingual-base',\n",
    "            # 'klue/roberta-base',\n",
    "            # 'snunlp/KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS',\n",
    "            # 'klue/bert-base',\n",
    "            ]\n",
    "data_routes = {'train_routes' : ['/content/final_data.csv',],\n",
    "               'valid_route' : '/content/text_dev.csv',\n",
    "               'test_route' : '/content/test.csv'}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--default_model_name', default = 'klue/roberta-small')\n",
    "parser.add_argument('--model_list', default = model_list, type = list)\n",
    "parser.add_argument('--batch_size', default = 32, type = int)\n",
    "parser.add_argument('--max_epoch', default = 3, type = int)\n",
    "parser.add_argument('--max_length', default = 160, type = int)\n",
    "parser.add_argument('--kf', default = 4, type = int)\n",
    "parser.add_argument('--data_routes', default = data_routes, type = dict)\n",
    "parser.add_argument('--weight_decay', default = 0.01, type = float)\n",
    "parser.add_argument('--eval_strategy', default = 'epoch', type = str)\n",
    "parser.add_argument('--save_strategy', default = 'epoch', type = str)\n",
    "parser.add_argument('--logging_dir', default = './logs', type = str)\n",
    "parser.add_argument('--logging_steps', default = 30, type = int)\n",
    "parser.add_argument('--test_route', default = test_route, type = str)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"./results/default\",\n",
    "    eval_strategy = args.eval_strategy,\n",
    "    save_strategy = args.save_strategy,\n",
    "    per_device_train_batch_size = args.batch_size,\n",
    "    per_device_eval_batch_size = args.batch_size,\n",
    "    num_train_epochs = args.max_epoch,\n",
    "    weight_decay = args.weight_decay,\n",
    "    logging_dir = args.logging_dir,\n",
    "    logging_steps = args.logging_steps,\n",
    "    report_to = \"wandb\",  \n",
    "    run_name = \"default\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'pearson'\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8408f7a9-228c-4a6f-b0df-3c4657cf5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_wandb():\n",
    "    !rm -rf /root/.cache/wandb\n",
    "    !rm -rf /root/.config/wandb\n",
    "    !rm -rf /root/.netrc\n",
    "    os.environ[\"WANDB_API_KEY\"] = \"ea26fff0d932bc74bbfad9fd507b292c67444c02\"\n",
    "    wandb.init(project=\"yonruka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a771b8c-1869-4b8c-8fb1-e56fd062e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_wandb()\n",
    "preds = {}\n",
    "# for model_name in model_list:\n",
    "#     preds[model_name] = train(model_name, args, training_args, kfold = True)\n",
    "for model_name in model_list:\n",
    "    preds[model_name] = train(model_name, args, training_args, kfold = False)\n",
    "\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0672b6-94e2-4bcc-8cac-5ca6521278a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
